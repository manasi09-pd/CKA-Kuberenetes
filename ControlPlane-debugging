# Kubernetes Control Plane Troubleshooting Guide

A comprehensive guide to debugging common Kubernetes control plane issues with practical solutions and diagnostic commands.

## Table of Contents
- [Scenario 1: API Server Connection Refused](#scenario-1-api-server-connection-refused)
- [Scenario 2: ETCD Server Issues](#scenario-2-etcd-server-issues)
- [Scenario 3: Certificate Path Errors](#scenario-3-certificate-path-errors)
- [Scenario 4: Controller Manager Failures](#scenario-4-controller-manager-failures)
- [Scenario 5: Scheduler Configuration Issues](#scenario-5-scheduler-configuration-issues)
- [Scenario 6: Certificate Expiration](#scenario-6-certificate-expiration)

---

## Scenario 1: API Server Connection Refused

### Problem Description
When running `kubectl` commands, you receive a connection refused error:
```
The connection to the server 172.30.1.2:6443 was refused - did you specify the right host or port?
```

### Symptoms
- `kubectl get pods` returns connection refused error
- API server is not responding on port 6443
- Unable to communicate with the cluster

### Diagnostic Steps

1. **Verify you're on the correct control plane node:**
   ```bash
   hostname -i
   ```

2. **Check if port 6443 is listening:**
   ```bash
   sudo ss -tlpn | grep 6443
   ```
   Expected output should show kube-apiserver listening on port 6443:
   ```
   LISTEN 0 4096 *:6443 *:* users:(("kube-apiserver",pid=15238,fd=3))
   ```

3. **Check running containers:**
   ```bash
   sudo crictl ps | grep kube-apiserver
   ```

4. **Check all containers including exited ones:**
   ```bash
   sudo crictl ps -a | grep kube-apiserver
   ```

5. **Review API server logs:**
   ```bash
   sudo crictl logs <container-id>
   # Or check pod logs directly:
   ls /var/log/pods/kube-system_kube-apiserver-*
   cat /var/log/pods/kube-system_kube-apiserver-*/kube-apiserver/*.log
   ```

### Common Root Causes

#### Invalid IP Address Configuration
**Error in logs:**
```
Error: invalid argument "172.30.1.2222" for "--advertise-address" flag: failed to parse IP: "172.30.1.2222"
```

**Solution:**
1. Edit the API server manifest:
   ```bash
   sudo vim /etc/kubernetes/manifests/kube-apiserver.yaml
   ```

2. Correct the `--advertise-address` parameter:
   ```yaml
   - --advertise-address=172.30.1.2  # Ensure this is a valid IP
   ```

3. Save the file. Kubelet will automatically restart the API server pod.

4. Verify the fix:
   ```bash
   sudo crictl ps | grep kube-apiserver
   kubectl get nodes
   ```

### Key Ports Reference
| Port | Service | Purpose |
|------|---------|---------|
| 6443 | kube-apiserver | Kubernetes API server |
| 2379 | etcd | etcd client requests |
| 2380 | etcd | etcd peer communication |
| 10250 | kubelet | Kubelet API |
| 10251 | kube-scheduler | Scheduler health checks |
| 10252 | kube-controller-manager | Controller manager health checks |

---

## Scenario 2: ETCD Server Issues

### Problem Description
`kubectl` commands hang or timeout. The API server cannot communicate with etcd.

### Symptoms
```
error from server (Timeout): the server was unable to return a response in the time allocated
```
- Commands get stuck and eventually timeout
- API server logs show etcd connection errors
- Read/write operations fail

### Diagnostic Steps

1. **Check etcd container status:**
   ```bash
   sudo crictl ps | grep etcd
   ```

2. **Check for exited etcd containers:**
   ```bash
   sudo crictl ps -a | grep etcd
   ```

3. **Review etcd logs:**
   ```bash
   sudo crictl logs <etcd-container-id>
   # Or:
   cat /var/log/pods/kube-system_etcd-controlplane-*/etcd/*.log
   ```

### Common Root Causes

#### Missing or Incorrect Certificate Path
**Error in logs:**
```
{"level":"error","ts":"2025-12-24T08:07:10.349229Z","caller":"embed/etcd.go:586",
"msg":"creating peer listener failed","error":"open /etc/kubernetes/pki/etcd/ca.crt: 
no such file or directory"}
```

**Solution:**
1. Verify certificate files exist:
   ```bash
   ls -la /etc/kubernetes/pki/etcd/
   ```

2. Edit the etcd manifest:
   ```bash
   sudo vim /etc/kubernetes/manifests/etcd.yaml
   ```

3. Correct the certificate paths:
   ```yaml
   spec:
     containers:
     - command:
       - etcd
       - --cert-file=/etc/kubernetes/pki/etcd/server.crt
       - --key-file=/etc/kubernetes/pki/etcd/server.key
       - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt  # Fix the path here
       - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
       - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
       - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
   ```

4. Verify volume mounts match the paths:
   ```yaml
   volumeMounts:
   - mountPath: /etc/kubernetes/pki/etcd
     name: etcd-certs
   volumes:
   - hostPath:
       path: /etc/kubernetes/pki/etcd
       type: DirectoryOrCreate
     name: etcd-certs
   ```

5. Save and wait for kubelet to restart the pod.

#### Data Directory Mismatch
**Error in logs:**
```
"started to purge a file /var/lib/etcd-data/member/snap"
```

**Solution:**
1. Edit the etcd manifest:
   ```bash
   sudo vim /etc/kubernetes/manifests/etcd.yaml
   ```

2. Ensure consistency between `--data-dir` and volume mount:
   ```yaml
   spec:
     containers:
     - command:
       - etcd
       - --data-dir=/var/lib/etcd  # Should match volume mount
     volumeMounts:
     - mountPath: /var/lib/etcd
       name: etcd-data
     volumes:
     - hostPath:
         path: /var/lib/etcd
         type: DirectoryOrCreate
       name: etcd-data
   ```

---

## Scenario 3: Certificate Path Errors

### Problem Description
Control plane components fail to start due to missing or incorrectly configured certificates.

### Symptoms
- Pods in CrashLoopBackOff state
- Authentication errors in logs
- "no such file or directory" errors for certificate files

### Diagnostic Steps

1. **Check all control plane component logs:**
   ```bash
   ls /var/log/pods/kube-system_kube-*
   ```

2. **Review specific component logs:**
   ```bash
   cat /var/log/pods/kube-system_kube-apiserver-*/kube-apiserver/*.log
   cat /var/log/pods/kube-system_etcd-*/etcd/*.log
   ```

3. **Verify certificate files exist:**
   ```bash
   ls -la /etc/kubernetes/pki/
   ls -la /etc/kubernetes/pki/etcd/
   ```

### Solution
Follow the certificate path correction steps outlined in Scenario 2.

**Important Note:** CNI (Container Network Interface) issues will affect workload pods but not control plane pods. Control plane pods run with `hostNetwork: true` and are static pods managed by kubelet.

---

## Scenario 4: Controller Manager Failures

### Problem Description
Deployments don't scale properly. Desired replica count doesn't match actual running pods.

### Symptoms
```bash
kubectl scale deployment nginx --replicas=4
kubectl get deployment nginx
# Output: nginx  3/4  3  3  10m
```
- Desired state: 4 replicas
- Current state: 3 replicas
- Controller manager pod not running

### Diagnostic Steps

1. **Check deployment status:**
   ```bash
   kubectl get deployments
   kubectl describe deployment <deployment-name>
   ```

2. **Check controller manager pod:**
   ```bash
   kubectl get pods -n kube-system | grep controller-manager
   ```

3. **Check controller manager logs:**
   ```bash
   kubectl logs -n kube-system kube-controller-manager-controlplane
   # Or:
   cat /var/log/pods/kube-system_kube-controller-manager-*/kube-controller-manager/*.log
   ```

4. **Check kubelet logs:**
   ```bash
   sudo journalctl -u kubelet -f
   ```

5. **Describe the controller manager pod:**
   ```bash
   kubectl describe pod kube-controller-manager-controlplane -n kube-system
   ```

### Common Root Causes

#### Binary Path Error
**Error in logs:**
```
OCI runtime create failed: unable to start container process: 
exec: "kube-controller-manager": executable file not found in $PATH
```

**Solution:**
1. Edit the controller manager manifest:
   ```bash
   sudo vim /etc/kubernetes/manifests/kube-controller-manager.yaml
   ```

2. Verify the command path is correct:
   ```yaml
   spec:
     containers:
     - command:
       - kube-controller-manager  # Ensure this is correct, not a typo
       # Should be: kube-controller-manager
       # Not: ube-controller-manager or kube-controler-manager
   ```

3. Common typos to check:
   - Missing 'k' at the beginning
   - Misspelled 'controller'
   - Missing hyphens

4. Save the file and verify:
   ```bash
   sudo crictl ps | grep controller-manager
   kubectl get pods -n kube-system
   ```

---

## Scenario 5: Scheduler Configuration Issues

### Problem Description
Pods remain in `Pending` state indefinitely without any resource constraints or affinity issues.

### Symptoms
```bash
kubectl get pods
# Output: pod-name  0/1  Pending  0  5m
```
- Pods stuck in Pending state
- No CrashLoopBackOff or ImagePullBackOff errors
- Scheduler pod not running properly

### Diagnostic Steps

1. **Check pod status:**
   ```bash
   kubectl get pods
   kubectl describe pod <pod-name>
   ```

2. **Check scheduler pod:**
   ```bash
   kubectl get pods -n kube-system | grep scheduler
   ```

3. **Review scheduler logs:**
   ```bash
   kubectl logs -n kube-system kube-scheduler-controlplane
   # Or:
   cat /var/log/pods/kube-system_kube-scheduler-*/kube-scheduler/*.log
   ```

### Common Root Causes

#### Missing or Incorrect Kubeconfig
**Error in logs:**
```
failed to get delegated authentication kubeconfig: 
stat /etc/kubernetes/scheduler.conf: no such file or directory
```

**Solution:**
1. Verify the scheduler.conf file exists:
   ```bash
   ls -la /etc/kubernetes/scheduler.conf
   ```

2. If missing, regenerate it:
   ```bash
   sudo kubeadm init phase kubeconfig scheduler
   ```

3. Edit the scheduler manifest if the path is incorrect:
   ```bash
   sudo vim /etc/kubernetes/manifests/kube-scheduler.yaml
   ```

4. Verify the kubeconfig path:
   ```yaml
   spec:
     containers:
     - command:
       - kube-scheduler
       - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
       - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
       - --kubeconfig=/etc/kubernetes/scheduler.conf
   ```

5. Ensure volume mounts are correct:
   ```yaml
   volumeMounts:
   - mountPath: /etc/kubernetes/scheduler.conf
     name: kubeconfig
     readOnly: true
   volumes:
   - hostPath:
       path: /etc/kubernetes/scheduler.conf
       type: FileOrCreate
     name: kubeconfig
   ```

---

## Scenario 6: Certificate Expiration

### Problem Description
Control plane components fail with x509 certificate errors after certificates expire.

### Symptoms
```
unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid
```
- API server authentication failures
- kubectl commands fail with certificate errors
- Control plane pods may be in CrashLoopBackOff

### Diagnostic Steps

1. **Check certificate expiration:**
   ```bash
   sudo kubeadm certs check-expiration
   ```
   
   **Expected output:**
   ```
   CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED
   admin.conf                 Dec 24, 2025 12:00 UTC   364d            ca                      no
   apiserver                  Dec 24, 2025 12:00 UTC   364d            ca                      no
   apiserver-etcd-client      Dec 24, 2025 12:00 UTC   364d            etcd-ca                 no
   apiserver-kubelet-client   Dec 24, 2025 12:00 UTC   364d            ca                      no
   controller-manager.conf    Dec 24, 2025 12:00 UTC   364d            ca                      no
   etcd-healthcheck-client    Dec 24, 2025 12:00 UTC   364d            etcd-ca                 no
   etcd-peer                  Dec 24, 2025 12:00 UTC   364d            etcd-ca                 no
   etcd-server                Dec 24, 2025 12:00 UTC   364d            etcd-ca                 no
   front-proxy-client         Dec 24, 2025 12:00 UTC   364d            front-proxy-ca          no
   scheduler.conf             Dec 24, 2025 12:00 UTC   364d            ca                      no
   ```

### Solution

#### Renew All Certificates

1. **Renew all certificates:**
   ```bash
   sudo kubeadm certs renew all
   ```

   **Expected output:**
   ```
   [renew] Reading configuration from the cluster...
   [renew] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
   
   certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed
   certificate for serving the Kubernetes API renewed
   certificate the apiserver uses to access etcd renewed
   certificate for the API server to connect to kubelet renewed
   certificate embedded in the kubeconfig file for the controller manager to use renewed
   certificate for liveness probes to healthcheck etcd renewed
   certificate for etcd nodes to communicate with each other renewed
   certificate for serving etcd renewed
   certificate for the front proxy client renewed
   certificate embedded in the kubeconfig file for the scheduler manager to use renewed
   
   Done renewing certificates. You must restart the kube-apiserver, kube-controller-manager, 
   kube-scheduler and etcd, so that they can use the new certificates.
   ```

2. **Regenerate kubeconfig files:**
   ```bash
   sudo kubeadm init phase kubeconfig all
   ```

3. **Update your local kubeconfig:**
   ```bash
   sudo cp /etc/kubernetes/admin.conf ~/.kube/config
   sudo chown $(id -u):$(id -g) ~/.kube/config
   ```

4. **Restart kubelet:**
   ```bash
   sudo systemctl restart kubelet
   ```

5. **Verify control plane pods restart (they should restart automatically):**
   ```bash
   watch kubectl get pods -n kube-system
   ```

6. **Verify the cluster is working:**
   ```bash
   kubectl get nodes
   kubectl get pods -A
   ```

7. **Verify new certificate expiration dates:**
   ```bash
   sudo kubeadm certs check-expiration
   ```

### Prevention

Set up automated certificate renewal:

1. **Enable kubelet certificate rotation:**
   ```bash
   sudo vim /var/lib/kubelet/config.yaml
   ```
   
   Add or verify:
   ```yaml
   rotateCertificates: true
   ```

2. **Set up a cron job for certificate renewal (optional):**
   ```bash
   sudo crontab -e
   ```
   
   Add:
   ```
   0 0 1 * * /usr/bin/kubeadm certs renew all && /usr/bin/systemctl restart kubelet
   ```

---

## Useful Diagnostic Commands

### Container Runtime Commands

```bash
# List running containers
sudo crictl ps

# List all containers (including stopped)
sudo crictl ps -a

# View container logs
sudo crictl logs <container-id>

# Inspect container
sudo crictl inspect <container-id>
```

### Network Diagnostics

```bash
# Check listening ports and processes
sudo ss -tlpn

# Check established connections
sudo ss -tnp state established

# Check specific port
sudo ss -tlpn | grep <port>
```

### Log Locations

```bash
# Kubelet logs
sudo journalctl -u kubelet -f
sudo journalctl -u kubelet --since "1 hour ago"

# Pod logs (static pods)
/var/log/pods/kube-system_<pod-name>_<pod-uid>/<container-name>/*.log

# Containerd logs
sudo journalctl -u containerd -f
```

### Manifest Files

All control plane component manifests are located at:
```
/etc/kubernetes/manifests/
├── etcd.yaml
├── kube-apiserver.yaml
├── kube-controller-manager.yaml
└── kube-scheduler.yaml
```

### Certificate Locations

```
/etc/kubernetes/pki/
├── apiserver.crt
├── apiserver.key
├── apiserver-kubelet-client.crt
├── apiserver-kubelet-client.key
├── ca.crt
├── ca.key
├── front-proxy-ca.crt
├── front-proxy-ca.key
├── front-proxy-client.crt
├── front-proxy-client.key
├── sa.key
├── sa.pub
└── etcd/
    ├── ca.crt
    ├── ca.key
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
```

---

## General Troubleshooting Workflow

1. **Identify the problem:**
   - Run kubectl commands to see what's failing
   - Check error messages carefully

2. **Check control plane pod status:**
   ```bash
   kubectl get pods -n kube-system
   sudo crictl ps -a
   ```

3. **Review logs:**
   - Check pod logs: `/var/log/pods/`
   - Check kubelet logs: `journalctl -u kubelet`
   - Check container logs: `crictl logs <container-id>`

4. **Verify configurations:**
   - Check manifest files: `/etc/kubernetes/manifests/`
   - Verify certificates: `/etc/kubernetes/pki/`
   - Check kubeconfig files: `/etc/kubernetes/*.conf`

5. **Apply fixes:**
   - Edit manifest files (kubelet will auto-restart pods)
   - Regenerate certificates or kubeconfig if needed
   - Restart services if necessary

6. **Verify the fix:**
   ```bash
   kubectl get nodes
   kubectl get pods -A
   kubectl cluster-info
   ```

---

## Best Practices

1. **Always backup before making changes:**
   ```bash
   sudo cp -r /etc/kubernetes /etc/kubernetes.backup.$(date +%Y%m%d)
   ```

2. **Use version control for manifest customizations:**
   - Track changes to manifest files
   - Document why changes were made

3. **Monitor certificate expiration:**
   - Set up alerts before certificates expire
   - Automate certificate renewal where possible

4. **Keep logs for troubleshooting:**
   - Configure log rotation appropriately
   - Retain logs for a reasonable period

5. **Test changes in non-production first:**
   - Validate fixes in a test environment
   - Document the resolution steps

6. **Regular health checks:**
   ```bash
   kubectl get componentstatuses  # Deprecated but still useful
   kubectl get --raw='/readyz?verbose'
   kubectl get --raw='/livez?verbose'
   ```

---

## Contributing

Found an issue or have a suggestion? Please open an issue or submit a pull request.

## License

This guide is provided as-is for educational purposes.

---

**Last Updated:** December 2025
