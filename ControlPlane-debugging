 ğŸš€ Kubernetes Control Plane Troubleshooting Guide
 A comprehensive guide to debugging common Kubernetes control plane issues with practical solutions and diagnostic commands for CKA/CKAD exam preparation and production troubleshooting.

ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Prerequisites](#-prerequisites)
- [Troubleshooting Scenarios](#-troubleshooting-scenarios)
  - [Scenario 1: API Server Connection Refused](#scenario-1-api-server-connection-refused)
  - [Scenario 2: ETCD Server Issues](#scenario-2-etcd-server-issues)
  - [Scenario 3: Certificate Path Errors](#scenario-3-certificate-path-errors)
  - [Scenario 4: Controller Manager Failures](#scenario-4-controller-manager-failures)
  - [Scenario 5: Scheduler Configuration Issues](#scenario-5-scheduler-configuration-issues)
  - [Scenario 6: Certificate Expiration](#scenario-6-certificate-expiration)


ğŸ¯ Overview

This repository contains detailed troubleshooting scenarios for Kubernetes control plane components. Each scenario includes:

- âœ… Problem Description - Clear explanation of the issue
- ğŸ” Symptoms - What you'll observe
- ğŸ› ï¸ Diagnostic Steps - Commands to identify the root cause
- ğŸ’¡ Solutions - Step-by-step fixes
- ğŸ“ Examples - Real-world command outputs

ğŸ“š Prerequisites

- Basic understanding of Kubernetes architecture
- Access to a Kubernetes cluster with root/sudo privileges
- Familiarity with command-line tools
- Knowledge of YAML configuration

Required Tools

kubectl    # Kubernetes CLI
crictl     # Container runtime interface CLI
kubeadm    # Kubernetes cluster management
journalctl # System logs viewer
ss         # Socket statistics


ğŸ”§ Troubleshooting Scenarios

Scenario 1: API Server Connection Refused

 ğŸ”´ Problem

When running `kubectl` commands, you receive a connection refused error:

The connection to the server 172.30.1.2:6443 was refused - did you specify the right host or port?

ğŸ¯ Symptoms

- âŒ `kubectl get pods` returns connection refused error
- âŒ API server is not responding on port 6443
- âŒ Unable to communicate with the cluster

ğŸ” Diagnostic Steps

**Step 1:** Verify you're on the correct control plane node

hostname -i

**Step 2:** Check if port 6443 is listening

```bash
sudo ss -tlpn | grep 6443
```

Expected output:
```
LISTEN 0 4096 *:6443 *:* users:(("kube-apiserver",pid=15238,fd=3))
```

**Step 3:** Check running containers

```bash
sudo crictl ps | grep kube-apiserver
```

**Step 4:** Check all containers including exited ones

```bash
sudo crictl ps -a | grep kube-apiserver
```

**Step 5:** Review API server logs

```bash
sudo crictl logs <container-id>
# Or check pod logs directly:
cat /var/log/pods/kube-system_kube-apiserver-*/kube-apiserver/*.log
```

#### âš ï¸ Common Root Cause: Invalid IP Address Configuration

**Error in logs:**
```
Error: invalid argument "172.30.1.2222" for "--advertise-address" flag: failed to parse IP: "172.30.1.2222"
```

#### âœ… Solution

**Step 1:** Edit the API server manifest

```bash
sudo vim /etc/kubernetes/manifests/kube-apiserver.yaml
```

**Step 2:** Correct the `--advertise-address` parameter

```yaml
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=172.30.1.2  # âœ… Ensure this is a valid IP
```

**Step 3:** Save the file. Kubelet will automatically restart the API server pod.

**Step 4:** Verify the fix

```bash
sudo crictl ps | grep kube-apiserver
kubectl get nodes
```

#### ğŸ“Š Key Ports Reference

| Port | Service | Purpose |
|------|---------|---------|
| 6443 | kube-apiserver | Kubernetes API server |
| 2379 | etcd | etcd client requests |
| 2380 | etcd | etcd peer communication |
| 10250 | kubelet | Kubelet API |
| 10257 | kube-controller-manager | Controller manager (secure) |
| 10259 | kube-scheduler | Scheduler (secure) |

</details>

---

### Scenario 2: ETCD Server Issues

<details>
<summary><strong>ğŸ“– Click to expand</strong></summary>

#### ğŸ”´ Problem

`kubectl` commands hang or timeout. The API server cannot communicate with etcd.

```bash
error from server (Timeout): the server was unable to return a response in the time allocated
```

#### ğŸ¯ Symptoms

- â³ Commands get stuck and eventually timeout
- âŒ API server logs show etcd connection errors
- âŒ Read/write operations fail

#### ğŸ” Diagnostic Steps

**Step 1:** Check etcd container status

```bash
sudo crictl ps | grep etcd
```

**Step 2:** Check for exited etcd containers

```bash
sudo crictl ps -a | grep etcd
```

Example output:
```
de68752865477 a3e246e9556e9 17 minutes ago Running etcd 2 36c3e4777cdf4 etcd-controlplane
c60c562293400 a3e246e9556e9 5 days ago     Exited  etcd 1 822c787c6f9fb etcd-controlplane
```

**Step 3:** Review etcd logs

```bash
sudo crictl logs <etcd-container-id>
# Or:
cat /var/log/pods/kube-system_etcd-controlplane-*/etcd/*.log
```

#### âš ï¸ Common Root Cause 1: Missing or Incorrect Certificate Path

**Error in logs:**
```json
{
  "level":"error",
  "ts":"2025-12-24T08:07:10.349229Z",
  "caller":"embed/etcd.go:586",
  "msg":"creating peer listener failed",
  "error":"open /etc/kubernetes/pki/etcd/ca.crt: no such file or directory"
}
```

#### âœ… Solution

**Step 1:** Verify certificate files exist

```bash
ls -la /etc/kubernetes/pki/etcd/
```

**Step 2:** Edit the etcd manifest

```bash
sudo vim /etc/kubernetes/manifests/etcd.yaml
```

**Step 3:** Correct the certificate paths

```yaml
spec:
  containers:
  - command:
    - etcd
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt  # âœ… Fix the path
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
```

**Step 4:** Verify volume mounts match the paths

```yaml
volumeMounts:
- mountPath: /etc/kubernetes/pki/etcd
  name: etcd-certs
volumes:
- hostPath:
    path: /etc/kubernetes/pki/etcd
    type: DirectoryOrCreate
  name: etcd-certs
```

#### âš ï¸ Common Root Cause 2: Data Directory Mismatch

**Error in logs:**
```
"started to purge a file /var/lib/etcd-data/member/snap"
```

#### âœ… Solution

Ensure consistency between `--data-dir` and volume mount:

```yaml
spec:
  containers:
  - command:
    - etcd
    - --data-dir=/var/lib/etcd  # âœ… Should match volume mount
  volumeMounts:
  - mountPath: /var/lib/etcd
    name: etcd-data
  volumes:
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data
```

#### ğŸ’¡ Important Note

> **CNI Issues vs Control Plane**: CNI (Container Network Interface) issues will affect workload pods but NOT control plane pods. Control plane pods run with `hostNetwork: true` and are static pods managed by kubelet.

</details>

---

### Scenario 3: Certificate Path Errors

<details>
<summary><strong>ğŸ“– Click to expand</strong></summary>

#### ğŸ”´ Problem

Control plane components fail to start due to missing or incorrectly configured certificates.

#### ğŸ¯ Symptoms

- ğŸ”„ Pods in CrashLoopBackOff state
- âŒ Authentication errors in logs
- ğŸ“ "no such file or directory" errors for certificate files

#### ğŸ” Diagnostic Steps

**Step 1:** Check all control plane component logs

```bash
ls /var/log/pods/kube-system_kube-*
```

**Step 2:** Review specific component logs

```bash
cat /var/log/pods/kube-system_kube-apiserver-*/kube-apiserver/*.log
cat /var/log/pods/kube-system_etcd-*/etcd/*.log
```

**Step 3:** Verify certificate files exist

```bash
ls -la /etc/kubernetes/pki/
ls -la /etc/kubernetes/pki/etcd/
```

#### âœ… Solution

Follow the certificate path correction steps outlined in **Scenario 2**.

#### ğŸ“‚ Certificate Directory Structure

```
/etc/kubernetes/pki/
â”œâ”€â”€ apiserver.crt
â”œâ”€â”€ apiserver.key
â”œâ”€â”€ apiserver-kubelet-client.crt
â”œâ”€â”€ apiserver-kubelet-client.key
â”œâ”€â”€ ca.crt
â”œâ”€â”€ ca.key
â”œâ”€â”€ front-proxy-ca.crt
â”œâ”€â”€ front-proxy-ca.key
â”œâ”€â”€ front-proxy-client.crt
â”œâ”€â”€ front-proxy-client.key
â”œâ”€â”€ sa.key
â”œâ”€â”€ sa.pub
â””â”€â”€ etcd/
    â”œâ”€â”€ ca.crt
    â”œâ”€â”€ ca.key
    â”œâ”€â”€ healthcheck-client.crt
    â”œâ”€â”€ healthcheck-client.key
    â”œâ”€â”€ peer.crt
    â”œâ”€â”€ peer.key
    â”œâ”€â”€ server.crt
    â””â”€â”€ server.key
```

</details>

---

### Scenario 4: Controller Manager Failures

<details>
<summary><strong>ğŸ“– Click to expand</strong></summary>

#### ğŸ”´ Problem

Deployments don't scale properly. Desired replica count doesn't match actual running pods.

#### ğŸ¯ Symptoms

```bash
kubectl scale deployment nginx --replicas=4
kubectl get deployment nginx
# Output: nginx  3/4  3  3  10m
```

- âŒ Desired state: 4 replicas
- âŒ Current state: 3 replicas
- ğŸ”„ Controller manager pod not running

#### ğŸ” Diagnostic Steps

**Step 1:** Check deployment status

```bash
kubectl get deployments
kubectl describe deployment <deployment-name>
```

**Step 2:** Check controller manager pod

```bash
kubectl get pods -n kube-system | grep controller-manager
```

**Step 3:** Check controller manager logs

```bash
kubectl logs -n kube-system kube-controller-manager-controlplane
# Or:
cat /var/log/pods/kube-system_kube-controller-manager-*/kube-controller-manager/*.log
```

**Step 4:** Check kubelet logs

```bash
sudo journalctl -u kubelet -f
```

**Step 5:** Describe the controller manager pod

```bash
kubectl describe pod kube-controller-manager-controlplane -n kube-system
```

#### âš ï¸ Common Root Cause: Binary Path Error

**Error in logs:**
```
OCI runtime create failed: unable to start container process: 
exec: "kube-controller-manager": executable file not found in $PATH
```

#### âœ… Solution

**Step 1:** Edit the controller manager manifest

```bash
sudo vim /etc/kubernetes/manifests/kube-controller-manager.yaml
```

**Step 2:** Verify the command path is correct

```yaml
spec:
  containers:
  - command:
    - kube-controller-manager  # âœ… Ensure this is correct
```

**Step 3:** Common typos to check

| âŒ Wrong | âœ… Correct |
|---------|-----------|
| `ube-controller-manager` | `kube-controller-manager` |
| `kube-controler-manager` | `kube-controller-manager` |
| `kube_controller_manager` | `kube-controller-manager` |

**Step 4:** Save and verify

```bash
sudo crictl ps | grep controller-manager
kubectl get pods -n kube-system
```

</details>

---

### Scenario 5: Scheduler Configuration Issues

<details>
<summary><strong>ğŸ“– Click to expand</strong></summary>

#### ğŸ”´ Problem

Pods remain in `Pending` state indefinitely without any resource constraints or affinity issues.

#### ğŸ¯ Symptoms

```bash
kubectl get pods
# Output:
NAME       READY   STATUS    RESTARTS   AGE
pod-name   0/1     Pending   0          5m
```

- â³ Pods stuck in Pending state
- âŒ No CrashLoopBackOff or ImagePullBackOff errors
- ğŸ”„ Scheduler pod not running properly

#### ğŸ” Diagnostic Steps

**Step 1:** Check pod status

```bash
kubectl get pods
kubectl describe pod <pod-name>
```

**Step 2:** Check scheduler pod

```bash
kubectl get pods -n kube-system | grep scheduler
```

**Step 3:** Review scheduler logs

```bash
kubectl logs -n kube-system kube-scheduler-controlplane
# Or:
cat /var/log/pods/kube-system_kube-scheduler-*/kube-scheduler/*.log
```

#### âš ï¸ Common Root Cause: Missing or Incorrect Kubeconfig

**Error in logs:**
```
failed to get delegated authentication kubeconfig: 
stat /etc/kubernetes/scheduler.conf: no such file or directory
```

#### âœ… Solution

**Step 1:** Verify the scheduler.conf file exists

```bash
ls -la /etc/kubernetes/scheduler.conf
```

**Step 2:** If missing, regenerate it

```bash
sudo kubeadm init phase kubeconfig scheduler
```

**Step 3:** Edit the scheduler manifest if the path is incorrect

```bash
sudo vim /etc/kubernetes/manifests/kube-scheduler.yaml
```

**Step 4:** Verify the kubeconfig path

```yaml
spec:
  containers:
  - command:
    - kube-scheduler
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
    - --kubeconfig=/etc/kubernetes/scheduler.conf
```

**Step 5:** Ensure volume mounts are correct

```yaml
volumeMounts:
- mountPath: /etc/kubernetes/scheduler.conf
  name: kubeconfig
  readOnly: true
volumes:
- hostPath:
    path: /etc/kubernetes/scheduler.conf
    type: FileOrCreate
  name: kubeconfig
```

</details>

---

### Scenario 6: Certificate Expiration

<details>
<summary><strong>ğŸ“– Click to expand</strong></summary>

#### ğŸ”´ Problem

Control plane components fail with x509 certificate errors after certificates expire.

#### ğŸ¯ Symptoms

```
unable to authenticate the request due to an error: 
x509: certificate has expired or is not yet valid
```

- âŒ API server authentication failures
- âŒ kubectl commands fail with certificate errors
- ğŸ”„ Control plane pods may be in CrashLoopBackOff

#### ğŸ” Diagnostic Steps

**Check certificate expiration:**

```bash
sudo kubeadm certs check-expiration
```

**Example output:**

```
CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY
admin.conf                 Dec 24, 2025 12:00 UTC   364d            ca
apiserver                  Dec 24, 2025 12:00 UTC   364d            ca
apiserver-etcd-client      Dec 24, 2025 12:00 UTC   364d            etcd-ca
apiserver-kubelet-client   Dec 24, 2025 12:00 UTC   364d            ca
controller-manager.conf    Dec 24, 2025 12:00 UTC   364d            ca
etcd-healthcheck-client    Dec 24, 2025 12:00 UTC   364d            etcd-ca
etcd-peer                  Dec 24, 2025 12:00 UTC   364d            etcd-ca
etcd-server                Dec 24, 2025 12:00 UTC   364d            etcd-ca
front-proxy-client         Dec 24, 2025 12:00 UTC   364d            front-proxy-ca
scheduler.conf             Dec 24, 2025 12:00 UTC   364d            ca
```

#### âœ… Solution: Renew All Certificates

**Step 1:** Renew all certificates

```bash
sudo kubeadm certs renew all
```

**Expected output:**

```
[renew] Reading configuration from the cluster...
[renew] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'

certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed
certificate for serving the Kubernetes API renewed
certificate the apiserver uses to access etcd renewed
certificate for the API server to connect to kubelet renewed
certificate embedded in the kubeconfig file for the controller manager to use renewed
certificate for liveness probes to healthcheck etcd renewed
certificate for etcd nodes to communicate with each other renewed
certificate for serving etcd renewed
certificate for the front proxy client renewed
certificate embedded in the kubeconfig file for the scheduler manager to use renewed

Done renewing certificates. You must restart the kube-apiserver, kube-controller-manager, 
kube-scheduler and etcd, so that they can use the new certificates.
```

**Step 2:** Regenerate kubeconfig files

```bash
sudo kubeadm init phase kubeconfig all
```

**Step 3:** Update your local kubeconfig

```bash
sudo cp /etc/kubernetes/admin.conf ~/.kube/config
sudo chown $(id -u):$(id -g) ~/.kube/config
```

**Step 4:** Restart kubelet

```bash
sudo systemctl restart kubelet
```

**Step 5:** Verify control plane pods restart

```bash
watch kubectl get pods -n kube-system
```

**Step 6:** Verify the cluster is working

```bash
kubectl get nodes
kubectl get pods -A
```

**Step 7:** Verify new certificate expiration dates

```bash
sudo kubeadm certs check-expiration
```

#### ğŸ”’ Prevention: Automated Certificate Renewal

**Option 1:** Enable kubelet certificate rotation

```bash
sudo vim /var/lib/kubelet/config.yaml
```

Add or verify:
```yaml
rotateCertificates: true
```

**Option 2:** Set up a cron job for certificate renewal

```bash
sudo crontab -e
```

Add:
```bash
# Renew certificates on the first day of each month
0 0 1 * * /usr/bin/kubeadm certs renew all && /usr/bin/systemctl restart kubelet
```

</details>

---

## ğŸ“– Quick Reference

### Essential Commands

#### Container Runtime Commands

```bash
# List running containers
sudo crictl ps

# List all containers (including stopped)
sudo crictl ps -a

# View container logs
sudo crictl logs <container-id>

# Inspect container
sudo crictl inspect <container-id>
```

#### Network Diagnostics

```bash
# Check listening ports and processes
sudo ss -tlpn

# Check established connections
sudo ss -tnp state established

# Check specific port
sudo ss -tlpn | grep <port>
```

#### Log Locations

```bash
# Kubelet logs
sudo journalctl -u kubelet -f
sudo journalctl -u kubelet --since "1 hour ago"

# Pod logs (static pods)
/var/log/pods/kube-system_<pod-name>_<pod-uid>/<container-name>/*.log

# Containerd logs
sudo journalctl -u containerd -f
```

### File Locations

#### Manifest Files

```
/etc/kubernetes/manifests/
â”œâ”€â”€ etcd.yaml
â”œâ”€â”€ kube-apiserver.yaml
â”œâ”€â”€ kube-controller-manager.yaml
â””â”€â”€ kube-scheduler.yaml
```

#### Kubeconfig Files

```
/etc/kubernetes/
â”œâ”€â”€ admin.conf
â”œâ”€â”€ controller-manager.conf
â”œâ”€â”€ kubelet.conf
â””â”€â”€ scheduler.conf
```

## ğŸ”„ General Troubleshooting Workflow

```mermaid
graph TD
    A[Identify Problem] --> B[Check Pod Status]
    B --> C{Pod Running?}
    C -->|No| D[Check Container Status]
    C -->|Yes| E[Check Logs]
    D --> F[Review crictl ps -a]
    F --> G[Check Container Logs]
    E --> H[Identify Root Cause]
    G --> H
    H --> I{Configuration Error?}
    I -->|Yes| J[Edit Manifest File]
    I -->|No| K{Certificate Error?}
    J --> L[Wait for Kubelet Restart]
    K -->|Yes| M[Fix Certificates]
    K -->|No| N[Check Other Components]
    L --> O[Verify Fix]
    M --> O
    N --> O
    O --> P{Issue Resolved?}
    P -->|No| A
    P -->|Yes| Q[Document Solution]
```

### Step-by-Step Process

1. **ğŸ” Identify the problem**
   - Run kubectl commands to see what's failing
   - Check error messages carefully

2. **ğŸ“Š Check control plane pod status**
   ```bash
   kubectl get pods -n kube-system
   sudo crictl ps -a
   ```

3. **ğŸ“ Review logs**
   - Pod logs: `/var/log/pods/`
   - Kubelet logs: `journalctl -u kubelet`
   - Container logs: `crictl logs <container-id>`

4. **âš™ï¸ Verify configurations**
   - Manifest files: `/etc/kubernetes/manifests/`
   - Certificates: `/etc/kubernetes/pki/`
   - Kubeconfig files: `/etc/kubernetes/*.conf`

5. **ğŸ”§ Apply fixes**
   - Edit manifest files (kubelet will auto-restart pods)
   - Regenerate certificates or kubeconfig if needed
   - Restart services if necessary

6. **âœ… Verify the fix**
   ```bash
   kubectl get nodes
   kubectl get pods -A
   kubectl cluster-info
   ```

## ğŸ’¡ Best Practices

### Before Making Changes

```bash
# Always backup before making changes
sudo cp -r /etc/kubernetes /etc/kubernetes.backup.$(date +%Y%m%d)

# Backup specific manifest
sudo cp /etc/kubernetes/manifests/kube-apiserver.yaml \
       /etc/kubernetes/manifests/kube-apiserver.yaml.backup
```

### Monitoring and Maintenance

#### Regular Health Checks

```bash
# Check component status
kubectl get componentstatuses  # Deprecated but still useful

# Check readiness
kubectl get --raw='/readyz?verbose'

# Check liveness
kubectl get --raw='/livez?verbose'

# Check nodes
kubectl get nodes -o wide

# Check all pods
kubectl get pods -A -o wide
```

#### Certificate Monitoring

```bash
# Check certificate expiration regularly
sudo kubeadm certs check-expiration

# Set up alert when certificates expire in 30 days
# Add to monitoring system or create a script
```
